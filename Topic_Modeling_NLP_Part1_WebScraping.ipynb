{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nafisur Rahman\n",
    "nafisur21@gmail.com<br>\n",
    "https://www.linkedin.com/in/nafisur-rahman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Problem in NLP\n",
    "* Identifying the genre of a book\n",
    "* Recognizing the themes or topics in an article\n",
    "* Topic modeling\n",
    "* Clustering text Data\n",
    "* Modeling Text Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling in NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering text data (Article) using Kmeans algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recognizing the themes or topics in an article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1:- Web Scraping\n",
    "Get all the article from given blogpost (http://doxydonkey.blogspot.in/) and save it as a tab seperated file into local system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraping\n",
    "Retrieving all the blogpost fron http://doxydonkey.blogspot.in/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we have to get all the url links that contain the blogpost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib # help to download a web page\n",
    "from bs4 import BeautifulSoup # help to parse the webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blogUrl = \"http://doxydonkey.blogspot.in\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading the webpage using urllib and save it in a variable named resp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resp=urllib.request.urlopen(blogUrl).read().decode('utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate a BeautifulSoup object on page which will help to creates a tree structure within the page based on structure of the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soup=BeautifulSoup(resp,'lxml')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tree stucture can be searched for a specific tags. Note:- Open the webpage in another tab. Then right click any where on the page and select \"inspect\". Find the name of required tag from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alla=soup.find_all('a')\n",
    "len(alla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in alla:\n",
    "    try:\n",
    "        print(i['title'])\n",
    "    except:\n",
    "        print('None')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the link of older posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Older Posts\n",
      "http://doxydonkey.blogspot.in/search?updated-max=2017-05-23T19:53:00-07:00&max-results=7\n"
     ]
    }
   ],
   "source": [
    "for i in alla:\n",
    "    try:\n",
    "        url=i['href']\n",
    "        title=i['title']\n",
    "        if title=='Older Posts':\n",
    "            print(title)\n",
    "            print(url)\n",
    "    except:\n",
    "        title=''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a function that will get all the url links from doxydonkey blogpost and save it into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "def getAllDoxyDonkeyPosts(url,lists):\n",
    "    resp=urllib.request.urlopen(url).read().decode('utf-8')\n",
    "    soup=BeautifulSoup(resp,'lxml')\n",
    "    alla=soup.find_all('a')\n",
    "    for a in alla:\n",
    "        try:\n",
    "            url=a['href']\n",
    "            title=a['title']\n",
    "            if title=='Older Posts':\n",
    "                #print(title)\n",
    "                #print(url)\n",
    "                lists.append(url)\n",
    "                getAllDoxyDonkeyPosts(url,lists)\n",
    "        except:\n",
    "            title=''\n",
    "    return\n",
    "\n",
    "lists=[]\n",
    "blogUrl = \"http://doxydonkey.blogspot.in\"\n",
    "getAllDoxyDonkeyPosts(blogUrl,lists)\n",
    "len(lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding all the articles from a url link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page=urllib.request.urlopen(blogUrl).read().decode('utf-8')\n",
    "soup=BeautifulSoup(page,'lxml')\n",
    "mydivs=soup.findAll('div',{'class':'post-body'})\n",
    "posts =[]\n",
    "for div in mydivs:\n",
    "    posts+=map(lambda p:p.text.encode('ascii', errors='replace').decode('utf-8').replace(\"?\",\" \"), div.findAll(\"li\"))\n",
    "len(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a function to get all the article from a given url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getDoxyDonkeyText(url):\n",
    "    page=urllib.request.urlopen(url).read().decode('utf-8')\n",
    "    soup=BeautifulSoup(page,'lxml')\n",
    "    mydivs=soup.findAll('div',{'class':'post-body'})\n",
    "    posts=[]\n",
    "    for div in mydivs:\n",
    "        posts+=map(lambda p:p.text.encode('ascii', errors='replace').decode('utf-8').replace(\"?\",\" \"), div.findAll(\"li\"))\n",
    "    return posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting all the article from all the links store in the lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doxyDonkeyPosts = []\n",
    "for link in lists:\n",
    "    doxyDonkeyPosts+=getDoxyDonkeyText(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2804"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doxyDonkeyPosts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doxyDonkeyPosts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a function that will get all the Article from doxydonkey blogpost and save into a tab separated file into local system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final take away"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2804"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def getAllDoxyDonkeyPosts(url,lists):\n",
    "    resp=urllib.request.urlopen(url).read().decode('utf-8')\n",
    "    soup=BeautifulSoup(resp,'lxml')\n",
    "    alla=soup.find_all('a')\n",
    "    for a in alla:\n",
    "        try:\n",
    "            url=a['href']\n",
    "            title=a['title']\n",
    "            if title=='Older Posts':\n",
    "                #print(title)\n",
    "                #print(url)\n",
    "                lists.append(url)\n",
    "                getAllDoxyDonkeyPosts(url,lists)\n",
    "        except:\n",
    "            title=''\n",
    "    return\n",
    "\n",
    "def getDoxyDonkeyText(url):\n",
    "    page=urllib.request.urlopen(url).read().decode('utf-8')\n",
    "    soup=BeautifulSoup(page,'lxml')\n",
    "    mydivs=soup.findAll('div',{'class':'post-body'})\n",
    "    posts=[]\n",
    "    for div in mydivs:\n",
    "        posts+=map(lambda p:p.text.encode('ascii', errors='replace').decode('utf-8').replace(\"?\",\" \"), div.findAll(\"li\"))\n",
    "    return posts\n",
    "\n",
    "blogUrl = \"http://doxydonkey.blogspot.in\"\n",
    "links = []\n",
    "getAllDoxyDonkeyPosts(blogUrl,links)\n",
    "doxyDonkeyPosts = []\n",
    "for link in links:\n",
    "    doxyDonkeyPosts+=getDoxyDonkeyText(link)\n",
    "\n",
    "len(doxyDonkeyPosts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SoftBank's $100 Billion Tech Fund Rankles VCs ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quora tests video answers to steal Q&amp;A from Yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pittsburgh Welcomed Uber s Driverless Car Expe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LeEco employees are being called to a Tuesday ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Why Did a Chinese Peroxide Company Pay $1 Bill...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                post\n",
       "0  SoftBank's $100 Billion Tech Fund Rankles VCs ...\n",
       "1  Quora tests video answers to steal Q&A from Yo...\n",
       "2  Pittsburgh Welcomed Uber s Driverless Car Expe...\n",
       "3  LeEco employees are being called to a Tuesday ...\n",
       "4  Why Did a Chinese Peroxide Company Pay $1 Bill..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(doxyDonkeyPosts,columns=['post'])\n",
    "df.to_csv('allposts.csv',sep='\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
